{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Data distillation-Copy1.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "qKdJRXZxB3nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "WCRpQxQeB3nz",
        "colab_type": "code",
        "colab": {},
        "outputId": "9eaa84b6-10eb-48d5-f50c-7d73d4154c0b"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Flatten,Activation\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from datetime import timedelta\n",
        "import time\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout, BatchNormalization\n",
        "from keras import regularizers\n",
        "import sys\n",
        "\n",
        "np.random.seed(666)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "XSL2tsJQB3oM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_weights_freeze_given_layers(model, weight_path, freeze_count):\n",
        "    model.load_weights(weight_path)\n",
        "    for layer in model.layers[:freeze_count]:\n",
        "        layer.trainable = False \n",
        "\n",
        "    return model\n",
        "# Get n randomly selected indices from the data \n",
        "def getTrainingDataIndicesOfSize(data, size):\n",
        "    chosen_indices = np.random.choice(np.arange(len(data)), size, replace=False)\n",
        "    return chosen_indices\n",
        "# Simple function to get a new sample from the given class\n",
        "def getNewDataOfClass(original_data, class_id):\n",
        "    chosen_index = np.random.choice(np.arange(len(original_data)), 1, replace=False)[0]\n",
        "    while(original_data[chosen_index][0] != class_id):\n",
        "        chosen_index = np.random.choice(np.arange(len(original_data)), 1, replace=False)[0]\n",
        "    return chosen_index\n",
        "# Add new samples of each class until the sample-size-per-class target is reached\n",
        "def fillLowSampleCountClassesTo(original_data, indices, target_count):\n",
        "    unique, counts = np.unique(original_data[indices], return_counts=True)\n",
        "    combined_stats = np.asarray((unique, counts)).T\n",
        "    for i in range(len(combined_stats)):\n",
        "        samples_needed = target_count - combined_stats[i][1]\n",
        "        while(samples_needed > 0):\n",
        "            potential_index = getNewDataOfClass(original_data, combined_stats[i][0])\n",
        "            if not (np.any(indices[:] == potential_index)):\n",
        "                indices = np.append(indices,potential_index)\n",
        "                samples_needed -= 1\n",
        "    return indices\n",
        "# Remove samples of each class until the sample-size-per-class target is reached\n",
        "def deleteSamplesOverPerClassLimit(original_data, indices, target_count):\n",
        "    unique, counts = np.unique(original_data[indices], return_counts=True)\n",
        "    combined_stats = np.asarray((unique, counts)).T\n",
        "    for i in range(len(combined_stats)):\n",
        "        samples_to_delete = combined_stats[i][1] - target_count\n",
        "        while(samples_to_delete > 0):\n",
        "            potential_index = np.random.choice(np.arange(len(indices)), 1, replace=False)[0]\n",
        "            if (original_data[indices[potential_index]] == combined_stats[i][0]):\n",
        "                indices = np.delete(indices,potential_index)\n",
        "                samples_to_delete -= 1\n",
        "    return indices\n",
        "# Get exactly n samples of each class randomly\n",
        "# 1. step: get n_class times sample_per_class indices (this gives approximately uniform distribution)\n",
        "# 2-3. step: correct sample-per-class differences by removing excess samples and adding to underrepresented classes\n",
        "def getRandomSamplesOfEachClassOfSize(data, n_classes, sample_per_class):\n",
        "    indices = getTrainingDataIndicesOfSize(data, n_classes*sample_per_class)\n",
        "    indices = fillLowSampleCountClassesTo(data, indices, sample_per_class)\n",
        "    indices = deleteSamplesOverPerClassLimit(data, indices, sample_per_class)\n",
        "  \n",
        "  \n",
        "    return indices\n",
        "\n",
        "def classification_report_dataframe(report, index=0):\n",
        "    report_data = []\n",
        "    lines = report.split('\\n')\n",
        "    row = {}\n",
        "\n",
        "    for line in lines[2:-5]:\n",
        "        row_data = line.split('      ')\n",
        "        class_idx = str(int(row_data[1]))\n",
        "        row[class_idx + 'precision'] = float(row_data[2])\n",
        "        row[class_idx + 'recall'] = float(row_data[3])\n",
        "        row[class_idx + 'f1_score'] = float(row_data[4])\n",
        "    \n",
        "    dataframe = pd.DataFrame(data=row, index=[index])\n",
        "    \n",
        "        return dataframe\n",
        "\n",
        "def timeElapsed(start_time):\n",
        "    current_time = time.time()\n",
        "    time_dif = current_time - start_time\n",
        "    return str(timedelta(seconds=int(round(time_dif))))\n",
        "\n",
        "def generateTrainingSet(x_supervised, y_supervised, x,y):\n",
        "    y = np.reshape(y,(40000,1))\n",
        "    x_train_supervised, x_valid_supervised, y_train_supervised, y_valid_supervised = train_test_split(x_supervised, y_supervised, test_size=0.2, shuffle= False)\n",
        "    x_train_unsupervised, x_valid_unsupervised, y_train_unsupervised, y_valid_unsupervised = train_test_split(x, y, test_size=0.005, shuffle= False)\n",
        "    \n",
        "\n",
        "    x_train_supervised = np.concatenate((x_train_supervised,x_train_supervised, x_train_supervised, x_train_supervised,x_train_supervised,x_train_supervised))\n",
        "    x_train_unsupervised = np.concatenate((x_train_unsupervised,x_train_unsupervised,x_train_unsupervised,x_train_unsupervised))\n",
        "    x_train = np.concatenate((x_train_supervised,x_train_unsupervised))\n",
        "    \n",
        "    y_train_supervised = np.concatenate((y_train_supervised,y_train_supervised, y_train_supervised, y_train_supervised,y_train_supervised,y_train_supervised))\n",
        "    y_train_unsupervised = np.concatenate((y_train_unsupervised,y_train_unsupervised,y_train_unsupervised,y_train_unsupervised))\n",
        "    y_train = np.concatenate((y_train_supervised,y_train_unsupervised))\n",
        "    \n",
        "    x_valid = np.concatenate((x_valid_supervised, x_valid_unsupervised))\n",
        "    y_valid = np.concatenate((y_valid_supervised, y_valid_unsupervised))\n",
        "    \n",
        "    return x_train, x_valid, y_train, y_valid  \n",
        "\n",
        "def train_gen(supervised_gen, unsupervised_gen):\n",
        "    while True:\n",
        "\n",
        "        sup_images, sup_labels = next(supervised_gen)\n",
        "        usup_images, usup_labels = next(unsupervised_gen)\n",
        "\n",
        "        images = np.concatenate((sup_images, usup_images))\n",
        "        labels = np.concatenate((sup_labels,usup_labels))\n",
        "\n",
        "        yield images,labels\n",
        "        \n",
        "def filtered_train_gen(supervised_gen, unsupervised_gen):\n",
        "    treshold = 0.85\n",
        "    while True:\n",
        "\n",
        "        sup_images, sup_labels = next(supervised_gen)\n",
        "        usup_images, usup_labels = next(unsupervised_gen)\n",
        "\n",
        "        img_tmp = np.concatenate((sup_images, usup_images))\n",
        "        lab_tmp = np.concatenate((sup_labels,usup_labels))\n",
        "        \n",
        "        images = []\n",
        "        labels = []\n",
        "        while len(labels) != 100:\n",
        "            for i in range(len(lab_tmp)):\n",
        "                if max(lab_tmp[i]) > treshold:\n",
        "                    images.append(img_tmp[i])\n",
        "                    label = np.argmax(lab_tmp[i])\n",
        "                    labels.append(to_categorical(label))\n",
        "\n",
        "            img_tmp, lab_tmp = next(unsupervised_gen)    \n",
        "        \n",
        "        yield images,labels\n",
        "        \n",
        "def filter_labels(x,predictions):\n",
        "    assert(len(x) == len(predictions))\n",
        "    treshold = 0.9\n",
        "    \n",
        "    images =[]\n",
        "    labels = []\n",
        "    \n",
        "    for i in range(len(predictions)):\n",
        "            if max(predictions[i]) > treshold:\n",
        "                images.append(x[i])\n",
        "                label = np.argmax(predictions[i])\n",
        "                labels.append(label)\n",
        "\n",
        "    return np.array(images), labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "-Y-W2ZldB3oW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getModel():\n",
        "    weight_decay = 1e-4\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3,3), padding='same', input_shape=(32, 32, 3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    \n",
        "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(lr=0.0001, decay=1e-6),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "yjNqKDzhB37b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_classes = 10\n",
        "n_iters = 10\n",
        "data_splits = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\n",
        "original_split = 0.1\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "x, x_supervised, y, y_supervised = train_test_split(X_train, Y_train, test_size=original_split, shuffle= False)\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(len(data_splits)):\n",
        "    x, x_supervised, y, y_supervised = train_test_split(X_train, Y_train, test_size=original_split, shuffle= False)\n",
        "    split = data_splits[i]\n",
        "    supervised_test_scores =  pd.DataFrame()\n",
        "    x_train, x_valid, y_train, y_valid = train_test_split(x_supervised, y_supervised, test_size=0.2, shuffle= True)\n",
        "\n",
        "    #Run several iterations as cross-validation    \n",
        "    for iteration in range(n_iters):\n",
        "        model = getModel()\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0,restore_best_weights=True)\n",
        "        \n",
        "        datagen = ImageDataGenerator(\n",
        "                rotation_range=15,\n",
        "                width_shift_range=0.1,\n",
        "                height_shift_range=0.1,\n",
        "                horizontal_flip=True)\n",
        "        \n",
        "        datagen.fit(x_train)\n",
        "        \n",
        "        #Supervised training\n",
        "        history = model.fit_generator(datagen.flow(x_train/255.0, to_categorical(y_train), batch_size=128), \n",
        "                                steps_per_epoch=x_train.shape[0] // 128, \n",
        "                                epochs=200,\n",
        "                                verbose=0, \n",
        "                                callbacks=[keras.callbacks.History(),early_stopping], \n",
        "                                validation_data=(x_valid / 255.0, to_categorical(y_valid)))\n",
        "        \n",
        "        score = model.evaluate(X_test / 255.0, to_categorical(Y_test), batch_size=128, verbose=0)\n",
        "        score = dict(zip(model.metrics_names, score))\n",
        "        print(score)\n",
        "        score_df = pd.DataFrame(data=score, index=[0])\n",
        "        y_pred = model.predict_classes(X_test)\n",
        "        report = classification_report(Y_test, y_pred)\n",
        "        report_df = classification_report_dataframe(report)\n",
        "        supervised_test_scores = supervised_test_scores.append(pd.concat([score_df, report_df], axis=1))\n",
        "        modelpath=\"models/\" + str(int(split*10)) + \"/\" + str(iteration) + \"-supervised_model.h5\"\n",
        "        model.save(modelpath)\n",
        "        \n",
        "        \n",
        "    \n",
        "    \n",
        "    supervised_logpath = \"logs/\" + \"test/\" + str(int(original_split*5000)) + \"/\" + str(int(split*10)) + \"/\"+ \"supervised.csv\"\n",
        "    supervised_test_scores.to_csv(supervised_logpath)\n",
        "    print(\"Finished with data split:\", split, \"Time elapsed so far:\", timeElapsed(start_time))\n",
        "\n",
        "    \n",
        "    print(\"Process over. Total time elapsed:\", timeElapsed(start_time))\n",
        "\n",
        "    dd_test_scores =  pd.DataFrame()\n",
        "\n",
        "    #Load supervised model, predict on unlabeled data\n",
        "    for iteration in range(n_iters):\n",
        "        model = getCompleteModel()\n",
        "        model_path =  \"models/\" + str(int(split*10)) + \"/\" + str(iteration) + \"-supervised_model.h5\"\n",
        "        model.load_weights(model_path)\n",
        "        transformations = [\"flip\", \"rotate\", \"wshift\", \"hshift\"]\n",
        "        predictions = np.zeros((40000,10))\n",
        "        for i in range(len(transformations)):\n",
        "            path = \"images/\" + transformations[i]\n",
        "            basic = ImageDataGenerator(rescale=1./255)\n",
        "            gen = basic.flow_from_directory(path, target_size=(32,32), class_mode=None, batch_size=64, shuffle=False)\n",
        "            current_preds = model.predict_generator(gen, verbose=0)\n",
        "            predictions = predictions + current_preds\n",
        "        predictions = predictions / len(transformations)\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        ref = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "        unsupervised_samples = int(split * 40000)\n",
        "\n",
        "        #Set up extended training set\n",
        "        x = x[:unsupervised_samples]\n",
        "        y = y[:unsupervised_samples]\n",
        "        predictions = predictions[:unsupervised_samples]\n",
        "\n",
        "        x_train_supervised, x_valid_supervised, y_train_supervised, y_valid_supervised = train_test_split(x_supervised, y_supervised, test_size=0.2, shuffle= False)\n",
        "        x_train_unsupervised, x_valid_unsupervised, y_train_unsupervised, y_valid_unsupervised = train_test_split(x, predictions, test_size=2/3*len(y_valid_supervised)/len(x), shuffle= False)\n",
        "\n",
        "        train_samples = len(x_train_supervised) + len(x_train_unsupervised)\n",
        "        validation_samples = len(x_valid_supervised) + len(x_valid_unsupervised)\n",
        "\n",
        "        supervised_gen_train = ref.flow(x_train_supervised,to_categorical(y_train_supervised),shuffle=True, seed=666, batch_size=60)\n",
        "        unsupervised_gen_train = ref.flow(x_train_unsupervised,to_categorical(y_train_unsupervised),shuffle=True, seed=666, batch_size=40)\n",
        "\n",
        "        supervised_gen_valid = ref.flow(x_valid_supervised,to_categorical(y_valid_supervised),shuffle=True, seed=666, batch_size=60)\n",
        "        unsupervised_gen_valid = ref.flow(x_valid_unsupervised,to_categorical(y_valid_unsupervised),shuffle=True, seed=666, batch_size=40)\n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0,restore_best_weights=True)\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.00025, decay=1e-6),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "        #Fine-tune model on extended dataset\n",
        "        history = model.fit_generator(train_gen(supervised_gen_train,unsupervised_gen_train),\n",
        "                                      steps_per_epoch=train_samples//100,\n",
        "                                      epochs=400,\n",
        "                                      verbose=0,\n",
        "                                      callbacks=[keras.callbacks.History(), early_stopping],\n",
        "                                      validation_data=train_gen(supervised_gen_valid,unsupervised_gen_valid),\n",
        "                                      validation_steps=validation_samples//100,\n",
        "                                      shuffle=True)\n",
        "        score = model.evaluate(X_test / 255.0, to_categorical(Y_test), batch_size=128, verbose=0)\n",
        "        score = dict(zip(model.metrics_names, score))\n",
        "        print(score)\n",
        "        score_df = pd.DataFrame(data=score, index=[0])\n",
        "        y_pred = model.predict_classes(X_test)\n",
        "        report = classification_report(Y_test, y_pred)\n",
        "        report_df = classification_report_dataframe(report)\n",
        "        dd_test_scores = dd_test_scores.append(pd.concat([score_df, report_df], axis=1))\n",
        "        modelpath=\"models/\" + str(int(split*10)) + \"/\" + str(iteration) + \"-dd_model.h5\"\n",
        "        model.save(modelpath)\n",
        "\n",
        "    dd_logpath = \"logs/\" + \"test/\" + str(int(original_split*5000)) + \"/\" + str(int(split*10)) + \"/\"+ \"dd.csv\"\n",
        "    dd_test_scores.to_csv(dd_logpath)\n",
        "    print(\"Finished with data split:\", split, \"Time elapsed so far:\", timeElapsed(start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "WXzL8kFlB37j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}