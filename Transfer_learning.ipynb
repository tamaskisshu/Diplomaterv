{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "Transfer learning - effect of target data size.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "rBKP4PCfNxBW",
        "colab_type": "code",
        "colab": {},
        "outputId": "6a483b7a-3a48-4102-b27d-268a640f6f8f"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Flatten,Activation\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout, BatchNormalization,GlobalAveragePooling2D\n",
        "from keras.models import load_model,Model\n",
        "import time\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout, BatchNormalization\n",
        "from keras import regularizers\n",
        "from sklearn.metrics import classification_report\n",
        "from datetime import timedelta\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "np.random.seed(666)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "wJ0OFCIkNxBl",
        "colab_type": "code",
        "colab": {},
        "outputId": "09e36a71-9fb5-4c42-9e38-ade17f61eb78"
      },
      "source": [
        "batch_size = 128\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        horizontal_flip=True,\n",
        "        validation_split = 0.2)\n",
        "\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        'img',\n",
        "        target_size=(32, 32),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset='training')\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        'img',\n",
        "        target_size=(32, 32),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        subset='validation')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 4000 images belonging to 10 classes.\n",
            "Found 1000 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "QWrSlq3KNxBz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_decay = 1e-4\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', input_shape=(32, 32, 3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "ytlVwRNYNxB8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(lr=0.00025, decay=1e-6),\n",
        "                  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Yyi3De_jNxCG",
        "colab_type": "code",
        "colab": {},
        "outputId": "a21899c2-6729-4886-8273-8dffe3a812f5"
      },
      "source": [
        "#Pretrain model on STL-10\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0,restore_best_weights=True)\n",
        "modelpath=\"models/complete_model-{epoch:02d}-{val_acc:.4f}-loss-{val_loss:.4f}.h5\"\n",
        "checkpoint = ModelCheckpoint(modelpath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = train_generator.samples // batch_size,\n",
        "    validation_data = validation_generator, \n",
        "    validation_steps = validation_generator.samples // batch_size,\n",
        "    epochs = 200,\n",
        "    verbose=2,\n",
        "    callbacks=[keras.callbacks.History(), checkpoint, early_stopping])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            " - 6s - loss: 2.6027 - acc: 0.2124 - val_loss: 1.9268 - val_acc: 0.3203\n",
            "Epoch 2/200\n",
            " - 6s - loss: 2.0446 - acc: 0.3171 - val_loss: 1.7302 - val_acc: 0.3796\n",
            "Epoch 3/200\n",
            " - 6s - loss: 1.8529 - acc: 0.3702 - val_loss: 1.6921 - val_acc: 0.3830\n",
            "Epoch 4/200\n",
            " - 6s - loss: 1.7580 - acc: 0.3959 - val_loss: 1.6080 - val_acc: 0.4243\n",
            "Epoch 5/200\n",
            " - 5s - loss: 1.6698 - acc: 0.4019 - val_loss: 1.6335 - val_acc: 0.4255\n",
            "Epoch 6/200\n",
            " - 6s - loss: 1.6423 - acc: 0.4221 - val_loss: 1.4868 - val_acc: 0.4656\n",
            "Epoch 7/200\n",
            " - 5s - loss: 1.5269 - acc: 0.4676 - val_loss: 1.5878 - val_acc: 0.4461\n",
            "Epoch 8/200\n",
            " - 5s - loss: 1.4637 - acc: 0.4753 - val_loss: 1.4325 - val_acc: 0.4702\n",
            "Epoch 9/200\n",
            " - 5s - loss: 1.4523 - acc: 0.4766 - val_loss: 1.4390 - val_acc: 0.4933\n",
            "Epoch 10/200\n",
            " - 6s - loss: 1.3993 - acc: 0.4889 - val_loss: 1.5003 - val_acc: 0.4702\n",
            "Epoch 11/200\n",
            " - 5s - loss: 1.3803 - acc: 0.5055 - val_loss: 1.3589 - val_acc: 0.5046\n",
            "Epoch 12/200\n",
            " - 5s - loss: 1.3366 - acc: 0.5265 - val_loss: 1.4747 - val_acc: 0.4656\n",
            "Epoch 13/200\n",
            " - 5s - loss: 1.2833 - acc: 0.5479 - val_loss: 1.3743 - val_acc: 0.5023\n",
            "Epoch 14/200\n",
            " - 5s - loss: 1.2614 - acc: 0.5488 - val_loss: 1.3870 - val_acc: 0.5115\n",
            "Epoch 15/200\n",
            " - 5s - loss: 1.2564 - acc: 0.5469 - val_loss: 1.3223 - val_acc: 0.5161\n",
            "Epoch 16/200\n",
            " - 5s - loss: 1.1907 - acc: 0.5685 - val_loss: 1.4053 - val_acc: 0.4989\n",
            "Epoch 17/200\n",
            " - 5s - loss: 1.1772 - acc: 0.5749 - val_loss: 1.3495 - val_acc: 0.5145\n",
            "Epoch 18/200\n",
            " - 6s - loss: 1.1523 - acc: 0.5884 - val_loss: 1.3011 - val_acc: 0.5195\n",
            "Epoch 19/200\n",
            " - 5s - loss: 1.1075 - acc: 0.6091 - val_loss: 1.2984 - val_acc: 0.5344\n",
            "Epoch 20/200\n",
            " - 5s - loss: 1.0948 - acc: 0.6026 - val_loss: 1.3434 - val_acc: 0.5470\n",
            "Epoch 21/200\n",
            " - 5s - loss: 1.0730 - acc: 0.6210 - val_loss: 1.2194 - val_acc: 0.5849\n",
            "Epoch 22/200\n",
            " - 5s - loss: 1.0463 - acc: 0.6316 - val_loss: 1.2759 - val_acc: 0.5459\n",
            "Epoch 23/200\n",
            " - 5s - loss: 1.0182 - acc: 0.6349 - val_loss: 1.2482 - val_acc: 0.5596\n",
            "Epoch 24/200\n",
            " - 5s - loss: 1.0101 - acc: 0.6419 - val_loss: 1.2809 - val_acc: 0.5550\n",
            "Epoch 25/200\n",
            " - 6s - loss: 0.9788 - acc: 0.6509 - val_loss: 1.3157 - val_acc: 0.5402\n",
            "Epoch 26/200\n",
            " - 6s - loss: 0.9719 - acc: 0.6645 - val_loss: 1.2101 - val_acc: 0.6044\n",
            "Epoch 27/200\n",
            " - 5s - loss: 0.9554 - acc: 0.6639 - val_loss: 1.2138 - val_acc: 0.5998\n",
            "Epoch 28/200\n",
            " - 5s - loss: 0.9365 - acc: 0.6583 - val_loss: 1.2066 - val_acc: 0.5745\n",
            "Epoch 29/200\n",
            " - 5s - loss: 0.9079 - acc: 0.6840 - val_loss: 1.1925 - val_acc: 0.6021\n",
            "Epoch 30/200\n",
            " - 5s - loss: 0.8933 - acc: 0.6908 - val_loss: 1.2067 - val_acc: 0.5883\n",
            "Epoch 31/200\n",
            " - 5s - loss: 0.8999 - acc: 0.6873 - val_loss: 1.1566 - val_acc: 0.6078\n",
            "Epoch 32/200\n",
            " - 5s - loss: 0.8483 - acc: 0.7041 - val_loss: 1.2267 - val_acc: 0.5768\n",
            "Epoch 33/200\n",
            " - 5s - loss: 0.8443 - acc: 0.7073 - val_loss: 1.2929 - val_acc: 0.5502\n",
            "Epoch 34/200\n",
            " - 6s - loss: 0.8659 - acc: 0.6937 - val_loss: 1.1485 - val_acc: 0.6101\n",
            "Epoch 35/200\n",
            " - 6s - loss: 0.8104 - acc: 0.7139 - val_loss: 1.1303 - val_acc: 0.6124\n",
            "Epoch 36/200\n",
            " - 6s - loss: 0.8173 - acc: 0.7092 - val_loss: 1.1328 - val_acc: 0.6227\n",
            "Epoch 37/200\n",
            " - 5s - loss: 0.7956 - acc: 0.7223 - val_loss: 1.2457 - val_acc: 0.5757\n",
            "Epoch 38/200\n",
            " - 5s - loss: 0.7685 - acc: 0.7298 - val_loss: 1.1403 - val_acc: 0.6319\n",
            "Epoch 39/200\n",
            " - 5s - loss: 0.7835 - acc: 0.7245 - val_loss: 1.2409 - val_acc: 0.5757\n",
            "Epoch 40/200\n",
            " - 5s - loss: 0.7653 - acc: 0.7313 - val_loss: 1.2161 - val_acc: 0.5929\n",
            "Epoch 41/200\n",
            " - 5s - loss: 0.7542 - acc: 0.7417 - val_loss: 1.1813 - val_acc: 0.6027\n",
            "Epoch 42/200\n",
            " - 6s - loss: 0.7388 - acc: 0.7400 - val_loss: 1.2093 - val_acc: 0.5917\n",
            "Epoch 43/200\n",
            " - 5s - loss: 0.7351 - acc: 0.7505 - val_loss: 1.0556 - val_acc: 0.6502\n",
            "Epoch 44/200\n",
            " - 5s - loss: 0.6971 - acc: 0.7596 - val_loss: 1.2818 - val_acc: 0.5791\n",
            "Epoch 45/200\n",
            " - 5s - loss: 0.6855 - acc: 0.7605 - val_loss: 1.1044 - val_acc: 0.6204\n",
            "Epoch 46/200\n",
            " - 5s - loss: 0.6902 - acc: 0.7646 - val_loss: 1.2480 - val_acc: 0.5894\n",
            "Epoch 47/200\n",
            " - 5s - loss: 0.6671 - acc: 0.7737 - val_loss: 1.1222 - val_acc: 0.6388\n",
            "Epoch 48/200\n",
            " - 5s - loss: 0.6789 - acc: 0.7610 - val_loss: 1.2672 - val_acc: 0.5826\n",
            "Epoch 49/200\n",
            " - 5s - loss: 0.6312 - acc: 0.7868 - val_loss: 1.1830 - val_acc: 0.6161\n",
            "Epoch 50/200\n",
            " - 6s - loss: 0.6583 - acc: 0.7817 - val_loss: 1.2114 - val_acc: 0.6158\n",
            "Epoch 51/200\n",
            " - 5s - loss: 0.6335 - acc: 0.7820 - val_loss: 1.1441 - val_acc: 0.6204\n",
            "Epoch 52/200\n",
            " - 5s - loss: 0.6290 - acc: 0.7907 - val_loss: 1.1011 - val_acc: 0.6365\n",
            "Epoch 53/200\n",
            " - 5s - loss: 0.6000 - acc: 0.7984 - val_loss: 1.1794 - val_acc: 0.6227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "GZGJvJleNxCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_path =  'models/base_model_dense-37-0.6422-loss-1.0778.h5'\n",
        "model.load_weights(model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "Bovm2ORENxCf",
        "colab_type": "code",
        "colab": {},
        "outputId": "9e65dabe-fffd-4e94-b32b-a1070c0b4918"
      },
      "source": [
        "freeze_steps = [2, 5, 10, 13, 17, 20, 24, 25]\n",
        "for i,layer in enumerate(model.layers[:]):\n",
        "    print(i,layer, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 <keras.layers.convolutional.Conv2D object at 0x7f9f26423a20> True\n",
            "1 <keras.layers.core.Activation object at 0x7f9f26423eb8> True\n",
            "2 <keras.layers.normalization.BatchNormalization object at 0x7f9f2643d208> True\n",
            "3 <keras.layers.convolutional.Conv2D object at 0x7f9f2643d438> True\n",
            "4 <keras.layers.core.Activation object at 0x7f9f2643da58> True\n",
            "5 <keras.layers.normalization.BatchNormalization object at 0x7f9f71653668> True\n",
            "6 <keras.layers.pooling.MaxPooling2D object at 0x7f9f715c8438> True\n",
            "7 <keras.layers.core.Dropout object at 0x7f9f715c88d0> True\n",
            "8 <keras.layers.convolutional.Conv2D object at 0x7f9f715eaac8> True\n",
            "9 <keras.layers.core.Activation object at 0x7f9f715eab38> True\n",
            "10 <keras.layers.normalization.BatchNormalization object at 0x7f9f71532f28> True\n",
            "11 <keras.layers.convolutional.Conv2D object at 0x7f9f714c59b0> True\n",
            "12 <keras.layers.core.Activation object at 0x7f9f714c5e48> True\n",
            "13 <keras.layers.pooling.MaxPooling2D object at 0x7f9f714a3eb8> True\n",
            "14 <keras.layers.core.Dropout object at 0x7f9f714a3b70> True\n",
            "15 <keras.layers.convolutional.Conv2D object at 0x7f9f7141ea20> True\n",
            "16 <keras.layers.core.Activation object at 0x7f9f7142e978> True\n",
            "17 <keras.layers.normalization.BatchNormalization object at 0x7f9f713d3ac8> True\n",
            "18 <keras.layers.convolutional.Conv2D object at 0x7f9f713e6ac8> True\n",
            "19 <keras.layers.core.Activation object at 0x7f9f713e6c18> True\n",
            "20 <keras.layers.pooling.MaxPooling2D object at 0x7f9f71344c18> True\n",
            "21 <keras.layers.core.Dropout object at 0x7f9f71344ba8> True\n",
            "22 <keras.layers.core.Flatten object at 0x7f9f7133cc88> True\n",
            "23 <keras.layers.core.Dense object at 0x7f9f712ceb38> True\n",
            "24 <keras.layers.core.Dense object at 0x7f9f712cecf8> True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "5SJ7OZfVNxCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def new_model():\n",
        "    weight_decay = 1e-4\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3,3), padding='same', input_shape=(32, 32, 3), kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "stuAdNiZNxCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load pretrained model weights and freeze <freeze_count> layers\n",
        "def load_weights_freeze_given_layers(model, weight_path, freeze_count):\n",
        "    model.load_weights(weight_path)\n",
        "    for layer in model.layers[:freeze_count]:\n",
        "        layer.trainable = False \n",
        "\n",
        "    return model\n",
        "# Get n randomly selected indices from the data \n",
        "def getTrainingDataIndicesOfSize(data, size):\n",
        "    chosen_indices = np.random.choice(np.arange(len(data)), size, replace=False)\n",
        "    return chosen_indices\n",
        "# Simple function to get a new sample from the given class\n",
        "def getNewDataOfClass(original_data, class_id):\n",
        "    chosen_index = np.random.choice(np.arange(len(original_data)), 1, replace=False)[0]\n",
        "    while(original_data[chosen_index][0] != class_id):\n",
        "        chosen_index = np.random.choice(np.arange(len(original_data)), 1, replace=False)[0]\n",
        "    return chosen_index\n",
        "# Add new samples of each class until the sample-size-per-class target is reached\n",
        "def fillLowSampleCountClassesTo(original_data, indices, target_count):\n",
        "    unique, counts = np.unique(original_data[indices], return_counts=True)\n",
        "    combined_stats = np.asarray((unique, counts)).T\n",
        "    for i in range(len(combined_stats)):\n",
        "        samples_needed = target_count - combined_stats[i][1]\n",
        "        while(samples_needed > 0):\n",
        "            potential_index = getNewDataOfClass(original_data, combined_stats[i][0])\n",
        "            if not (np.any(indices[:] == potential_index)):\n",
        "                indices = np.append(indices,potential_index)\n",
        "                samples_needed -= 1\n",
        "    return indices\n",
        "# Remove samples of each class until the sample-size-per-class target is reached\n",
        "def deleteSamplesOverPerClassLimit(original_data, indices, target_count):\n",
        "    unique, counts = np.unique(original_data[indices], return_counts=True)\n",
        "    combined_stats = np.asarray((unique, counts)).T\n",
        "    for i in range(len(combined_stats)):\n",
        "        samples_to_delete = combined_stats[i][1] - target_count\n",
        "        while(samples_to_delete > 0):\n",
        "            potential_index = np.random.choice(np.arange(len(indices)), 1, replace=False)[0]\n",
        "            if (original_data[indices[potential_index]] == combined_stats[i][0]):\n",
        "                indices = np.delete(indices,potential_index)\n",
        "                samples_to_delete -= 1\n",
        "    return indices\n",
        "# Get exactly n samples of each class randomly\n",
        "# 1. step: get n_class times sample_per_class indices (this gives approximately uniform distribution)\n",
        "# 2-3. step: correct sample-per-class differences by removing excess samples and adding to underrepresented classes\n",
        "def getRandomSamplesOfEachClassOfSize(data, n_classes, sample_per_class):\n",
        "    indices = getTrainingDataIndicesOfSize(data, n_classes*sample_per_class)\n",
        "    indices = fillLowSampleCountClassesTo(data, indices, sample_per_class)\n",
        "    indices = deleteSamplesOverPerClassLimit(data, indices, sample_per_class)\n",
        "  \n",
        "  \n",
        "    return indices\n",
        "\n",
        "def classification_report_dataframe(report, index=0):\n",
        "    report_data = []\n",
        "    lines = report.split('\\n')\n",
        "    row = {}\n",
        "\n",
        "    for line in lines[2:-5]:\n",
        "        row_data = line.split('      ')\n",
        "        class_idx = str(int(row_data[1]))\n",
        "        row[class_idx + 'precision'] = float(row_data[2])\n",
        "        row[class_idx + 'recall'] = float(row_data[3])\n",
        "        row[class_idx + 'f1_score'] = float(row_data[4])\n",
        "    \n",
        "    dataframe = pd.DataFrame(data=row, index=[index])\n",
        "    \n",
        "    \n",
        "    return dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "2zCwLvOENxC6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Experiments on CIFAR-10\n",
        "freeze_steps =  [0, 2, 5, 10, 13, 17, 20, 23]\n",
        "\n",
        "sample_counts = [50,100,200,400,700,1000,1500,2000,3000,4000,5000]\n",
        "\n",
        "n_classes = 10\n",
        "n_iters = 10\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "start_time = time.time()\n",
        "\n",
        "weight_path = 'models/complete_model_dense-32-0.6353-loss-1.0800.h5'\n",
        "\n",
        "for i in range(len(sample_counts)):\n",
        "    sample_size = sample_counts[i]\n",
        "    print(\"--- SAMPLE SIZE:\", sample_size, \"---\")\n",
        "    for j in range(len(freeze_steps)):\n",
        "        layers_to_freeze = freeze_steps[j]\n",
        "        test_scores =  pd.DataFrame()\n",
        "        print(\"Frozen layer count: \" , layers_to_freeze)\n",
        "\n",
        "        for iteration in range(n_iters):\n",
        "                print(\"\\t Iteration: \", iteration)\n",
        "                model = new_model()\n",
        "                model = load_weights_freeze_given_layers(model, weight_path, layers_to_freeze)\n",
        "                model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=Adam(lr=0.00025, decay=1e-6),\n",
        "                      metrics=['accuracy'])\n",
        "                indices = getRandomSamplesOfEachClassOfSize(Y_train, n_classes, sample_size)\n",
        "                x_train = X_train[indices]\n",
        "                y_train = Y_train[indices]\n",
        "                x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, shuffle= True)\n",
        "                modelpath=\"models/\" + str(sample_size) + \"/\" + str(layers_to_freeze) + \"/\" + str(iteration) + \"-base_model.h5\"\n",
        "                checkpoint = ModelCheckpoint(modelpath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
        "\n",
        "                train_logpath = \"logs/\" + \"training/\" + str(sample_size) + \"/\" + str(layers_to_freeze) + \"/\" + str(iteration) + \"-base_model.csv\"\n",
        "                train_logger = CSVLogger(train_logpath, append=False, separator=';')\n",
        "\n",
        "                early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0,restore_best_weights=True)\n",
        "        \n",
        "                history = model.fit(x_train / 255.0, to_categorical(y_train),\n",
        "                          batch_size=128,\n",
        "                          shuffle=True,\n",
        "                          epochs=200,\n",
        "                          validation_data=(x_valid / 255.0, to_categorical(y_valid)),\n",
        "                          callbacks=[keras.callbacks.History(), checkpoint, train_logger, early_stopping],\n",
        "                                   verbose = 0)\n",
        "                score = model.evaluate(X_test / 255.0, to_categorical(Y_test), batch_size=128, verbose=0)\n",
        "                score = dict(zip(model.metrics_names, score))\n",
        "                score_df = pd.DataFrame(data=score, index=[0])\n",
        "                y_pred = model.predict_classes(X_test)\n",
        "                report = classification_report(Y_test, y_pred)\n",
        "                report_df = classification_report_dataframe(report)\n",
        "                test_scores = test_scores.append(pd.concat([score_df, report_df], axis=1))\n",
        "        test_logpath = \"logs/\" + \"/test/\" + str(sample_size) + \"/\" + str(layers_to_freeze) + \"/\"+ \"base_model.csv\"\n",
        "        test_scores.to_csv(test_logpath)\n",
        "        print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(test_scores['acc']), np.std(test_scores['acc'])))\n",
        "end_time = time.time()\n",
        "time_dif = end_time - start_time\n",
        "print(\"Total elapsed time: \",  str(timedelta(seconds=int(round(time_dif)))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "xItLbta-NxDK",
        "colab_type": "code",
        "colab": {},
        "outputId": "5a0cabed-043b-4076-9a96-676094c139e7"
      },
      "source": [
        "train_data_dir = 'analyzer_calc_data/train'\n",
        "validation_data_dir = 'analyzer_calc_data/validation'\n",
        "test_data_dir = 'analyzer_calc_data/test'\n",
        "\n",
        "nb_train_samples = 2146\n",
        "nb_validation_samples = 604\n",
        "nb_test_samples = 604\n",
        "nb_epoch = 60\n",
        "pretrain_epoch = 20\n",
        "batch_size = 12\n",
        "\n",
        "img_height = 299\n",
        "img_width = 299\n",
        "train_datagen = ImageDataGenerator(\n",
        "rotation_range= 30,\n",
        "zoom_range = 0.3,\n",
        "width_shift_range=0.2,\n",
        "height_shift_range=0.2,\n",
        "horizontal_flip=True,\n",
        "vertical_flip = True,\n",
        "rescale = 1./255,\n",
        "fill_mode='nearest')\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_data_dir,\n",
        "target_size = (img_width, img_height),\n",
        "class_mode = 'binary',\n",
        "batch_size = batch_size,\n",
        "shuffle = True,\n",
        "seed = 666)\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "validation_data_dir,\n",
        "target_size = (img_width, img_height),\n",
        "class_mode = 'binary',\n",
        "batch_size = batch_size,\n",
        "shuffle = True,\n",
        "seed = 666)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "test_data_dir,\n",
        "target_size = (img_width, img_height),\n",
        "class_mode = 'binary',\n",
        "batch_size = batch_size,\n",
        "shuffle = True,\n",
        "seed = 666)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2146 images belonging to 2 classes.\n",
            "Found 604 images belonging to 2 classes.\n",
            "Found 604 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "388_V0XLNxDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Freeze steps corresponding to Inception modules\n",
        "freeze_steps =  [41, 64, 87, 101, 133, 165, 197, 229, 249]\n",
        "\n",
        "\n",
        "n_iters = 4\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "for j in range(len(freeze_steps)):\n",
        "    layers_to_freeze = freeze_steps[j]\n",
        "    test_scores =  pd.DataFrame()\n",
        "    print(\"Frozen layer count: \" , layers_to_freeze)\n",
        "\n",
        "    for iteration in range(n_iters):\n",
        "            print(\"\\t Iteration: \", iteration)\n",
        "            loaded_model = InceptionV3(include_top=False, weights='imagenet')\n",
        "            top = loaded_model.output\n",
        "            top = GlobalAveragePooling2D()(top)\n",
        "            top = Dense(1248, activation='relu')(top)\n",
        "            top = Dropout(0.4)(top)\n",
        "\n",
        "\n",
        "            classifier = Dense(1, activation='sigmoid')(top)\n",
        "            model = Model(inputs=loaded_model.input, outputs=classifier)\n",
        "            for layer in model.layers[:layers_to_freeze]:\n",
        "                layer.trainable = False\n",
        "            adam = Adam(lr=0.00035, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "            model.compile(loss='binary_crossentropy',\n",
        "                          optimizer= adam,\n",
        "                          metrics=['accuracy'])\n",
        "            \n",
        "            modelpath=\"calc_models/\" + str(layers_to_freeze) + \"/\" + str(iteration) + \"-inception_model.h5\"\n",
        "            checkpoint = ModelCheckpoint(modelpath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
        "\n",
        "            train_logpath = \"calc_logs/\" + \"training/\"  + str(layers_to_freeze) + \"/\" + str(iteration) + \"-inception_model.csv\"\n",
        "            train_logger = CSVLogger(train_logpath, append=False, separator=';')\n",
        "\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0,restore_best_weights=True)\n",
        "            \n",
        "            history = model.fit_generator(\n",
        "                train_generator,\n",
        "                steps_per_epoch = nb_train_samples / batch_size,\n",
        "                epochs =2, #epochs =nb_epoch,\n",
        "                verbose = 2,\n",
        "                validation_data = validation_generator,\n",
        "                validation_steps = nb_validation_samples /batch_size,\n",
        "                callbacks=[keras.callbacks.History(), checkpoint, train_logger, early_stopping])\n",
        "\n",
        "            score = model.evaluate_generator(test_generator, nb_test_samples)\n",
        "            score = dict(zip(model.metrics_names, score))\n",
        "            print(score)\n",
        "            score_df = pd.DataFrame(data=score, index=[0])\n",
        "\n",
        "            test_scores = test_scores.append(score_df)\n",
        "    test_logpath = \"calc_logs/\" + \"test\" + \"/\" + str(layers_to_freeze) + \"/\"+ \"inception_model.csv\"\n",
        "    test_scores.to_csv(test_logpath)\n",
        "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(test_scores['acc']), np.std(test_scores['acc'])))\n",
        "end_time = time.time()\n",
        "time_dif = end_time - start_time\n",
        "print(\"Total elapsed time: \",  str(timedelta(seconds=int(round(time_dif)))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": true,
        "editable": true,
        "id": "H7JL4TG_NxDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}