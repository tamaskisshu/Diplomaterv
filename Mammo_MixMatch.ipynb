{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9do8RAcuQxkK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import json\n",
        "import gc\n",
        "from keras_radam.training import RAdamOptimizer\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqh2835bQzE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_batch_size = 8\n",
        "\n",
        "def create_datagen():\n",
        "    return tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        validation_split=0.9535,\n",
        "        rescale=1./255)\n",
        "\n",
        "def create_unlabeled_gen():\n",
        "    return tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "        directory=r'mammo-calc\\analyzer_calc_data\\train',\n",
        "        class_mode=None,\n",
        "        batch_size=generator_batch_size,\n",
        "        target_size=(256, 256),\n",
        "        shuffle=True\n",
        "    \n",
        "    )\n",
        "\n",
        "def create_flow(datagen, subset):\n",
        "    return datagen.flow_from_directory(\n",
        "        directory=r'mammo-calc\\analyzer_calc_data\\train',\n",
        "        batch_size=generator_batch_size,\n",
        "        target_size=(256, 256),\n",
        "        shuffle=True,\n",
        "        subset=subset,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "def create_test_flow(generator, directory):\n",
        "    return generator.flow_from_directory(\n",
        "        directory=directory, \n",
        "        batch_size=generator_batch_size,\n",
        "        target_size=(256, 256),\n",
        "        shuffle=False,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "def create_testgen():\n",
        "    return tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "data_generator = create_datagen()\n",
        "test_gen = create_testgen()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTGtfOr-Q0jI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = create_flow(data_generator, 'training')\n",
        "unlabeled_generator = create_flow(data_generator, 'validation')\n",
        "test_generator = create_test_flow(test_gen, r'mammo-calc\\analyzer_calc_data\\test')\n",
        "validation_generator = create_test_flow(test_gen, r'mammo-calc\\analyzer_calc_data\\validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccnK9TLPQ28_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://raw.githubusercontent.com/ntozer/mixmatch-tensorflow2.0/master/model.py\n",
        "class Residual3x3Unit(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels_in, channels_out, stride, droprate=0., activate_before_residual=False):\n",
        "        super(Residual3x3Unit, self).__init__()\n",
        "        self.bn_0 = tf.keras.layers.BatchNormalization(momentum=0.999)\n",
        "        self.relu_0 = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "        self.conv_0 = tf.keras.layers.Conv2D(channels_out, kernel_size=3, strides=stride, padding='same', use_bias=False)\n",
        "        self.bn_1 = tf.keras.layers.BatchNormalization(momentum=0.999)\n",
        "        self.relu_1 = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(channels_out, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
        "        self.downsample = channels_in != channels_out\n",
        "        self.shortcut = tf.keras.layers.Conv2D(channels_out, kernel_size=1, strides=stride, use_bias=False)\n",
        "        self.activate_before_residual = activate_before_residual\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=droprate)\n",
        "        self.droprate = droprate\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=True):\n",
        "        if self.downsample and self.activate_before_residual:\n",
        "            x = self.relu_0(self.bn_0(x, training=training))\n",
        "        elif not self.downsample:\n",
        "            out = self.relu_0(self.bn_0(x, training=training))\n",
        "        out = self.relu_1(self.bn_1(self.conv_0(x if self.downsample else out), training=training))\n",
        "        if self.droprate > 0.:\n",
        "            out = self.dropout(out)\n",
        "        out = self.conv_1(out)\n",
        "        return out + (self.shortcut(x) if self.downsample else x)\n",
        "\n",
        "\n",
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_units, channels_in, channels_out, unit, stride, droprate=0., activate_before_residual=False):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.units = self._build_unit(n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual)\n",
        "\n",
        "    def _build_unit(self, n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual):\n",
        "        units = []\n",
        "        for i in range(n_units):\n",
        "            units.append(unit(channels_in if i == 0 else channels_out, channels_out, stride if i == 0 else 1, droprate, activate_before_residual))\n",
        "        return units\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=True):\n",
        "        for unit in self.units:\n",
        "            x = unit(x, training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class WideResNet(tf.keras.Model):\n",
        "    def __init__(self, num_classes, depth=28, width=2, droprate=0., input_shape=(None, 32, 32, 3), **kwargs):\n",
        "        super(WideResNet, self).__init__(input_shape, **kwargs)\n",
        "        assert (depth - 4) % 6 == 0\n",
        "        N = int((depth - 4) / 6)\n",
        "        channels = [16, 16 * width, 32 * width, 64 * width]\n",
        "\n",
        "        self.conv_0 = tf.keras.layers.Conv2D(channels[0], kernel_size=3, strides=1, padding='same', use_bias=False)\n",
        "        self.block_0 = ResidualBlock(N, channels[0], channels[1], Residual3x3Unit, 1, droprate, True)\n",
        "        self.block_1 = ResidualBlock(N, channels[1], channels[2], Residual3x3Unit, 2, droprate)\n",
        "        self.block_2 = ResidualBlock(N, channels[2], channels[3], Residual3x3Unit, 2, droprate)\n",
        "        self.bn_0 = tf.keras.layers.BatchNormalization(momentum=0.999)\n",
        "        self.relu_0 = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "        self.avg_pool = tf.keras.layers.AveragePooling2D((8, 8), (1, 1))\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=True):\n",
        "        x = inputs\n",
        "        x = self.conv_0(x)\n",
        "        x = self.block_0(x, training=training)\n",
        "        x = self.block_1(x, training=training)\n",
        "        x = self.block_2(x, training=training)\n",
        "        x = self.relu_0(self.bn_0(x, training=training))\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Soao6hN3Q4P_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Supervised benchmark\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "\n",
        "minimum_val_loss = None\n",
        "BATCH_SIZE = 8\n",
        "# Keep results for plotting\n",
        "train_loss_results = []\n",
        "train_accuracy_results = []\n",
        "model = WideResNet(1, depth=16, width=2)\n",
        "model.build(input_shape=(None, 256, 256, 3))\n",
        "num_epochs = 400\n",
        "\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam(0.00001) #\n",
        "epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "epoch_accuracy = tf.keras.metrics.BinaryAccuracy()\n",
        "avg_val_loss = tf.keras.metrics.Mean()\n",
        "tmp_loss = tf.keras.metrics.Mean()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "global_step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_start = tf.timestamp(name=None)\n",
        "\n",
        "    train_step = 1\n",
        "    valid_step = 1\n",
        "\n",
        "    # Training loop - using batches of 32\n",
        "    for x, y in train_generator:\n",
        "        if train_step%(400//BATCH_SIZE)==0:\n",
        "            break\n",
        "        # Optimize the model\n",
        "        with tf.GradientTape() as tape:\n",
        "                logits = model(x)\n",
        "                loss_value = tf.keras.backend.binary_crossentropy(target=y, output=tf.squeeze(logits, axis=1), from_logits=True) \n",
        "                loss_value = tf.reduce_mean(loss_value)\n",
        "    \n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        # Track progress\n",
        "        epoch_loss_avg(loss_value)  # Add current batch loss\n",
        "        # Compare predicted label to actual label\n",
        "        epoch_accuracy(y, tf.nn.sigmoid(logits))\n",
        "        train_step = train_step + 1\n",
        "        global_step = global_step + 1\n",
        "        tmp_loss(loss_value)\n",
        "\n",
        "    for x_batch_val, y_batch_val in validation_generator:\n",
        "        if valid_step%(604//BATCH_SIZE) == 0:\n",
        "            break\n",
        "        val_logits = model(x_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        val_acc_metric(y_batch_val, tf.nn.sigmoid(val_logits))\n",
        "        val_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_batch_val, logits=tf.squeeze(val_logits, axis=1))\n",
        "        avg_val_loss(tf.reduce_mean(val_loss))\n",
        "        valid_step = valid_step + 1\n",
        "    epoch_end = tf.timestamp(name=None)\n",
        "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}, Global steps: {:.5f}\".format(epoch,\n",
        "                                                                epoch_loss_avg.result(),\n",
        "                                                                epoch_accuracy.result(),\n",
        "                                                                global_step))\n",
        "    print(f'Validation loss: {avg_val_loss.result():2f}, Validation Accuracy: {val_acc_metric.result():2%}, Time:: {str(int((epoch_end.numpy()-epoch_start.numpy())))}s')\n",
        "\n",
        "    epoch_loss_avg.reset_states()    \n",
        "    epoch_accuracy.reset_states()\n",
        "    avg_val_loss.reset_states()\n",
        "    val_acc_metric.reset_states()\n",
        "\n",
        "    tmp_loss.reset_states()\n",
        "\n",
        "    # End epoch\n",
        "    train_loss_results.append(epoch_loss_avg.result())\n",
        "    train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrhW7LxBRKwY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_batch_size = 8\n",
        "\n",
        "def create_datagen():\n",
        "    return tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,\n",
        "        validation_split=0.9535,\n",
        "        rescale=1./255)\n",
        "\n",
        "def create_unlabeled_gen():\n",
        "    return tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "        directory=r'mammo-calc\\analyzer_calc_data\\train',\n",
        "        class_mode=None,\n",
        "        batch_size=generator_batch_size,\n",
        "        target_size=(256, 256),\n",
        "        shuffle=True\n",
        "    \n",
        "    )\n",
        "\n",
        "def create_flow(datagen, subset):\n",
        "    return datagen.flow_from_directory(\n",
        "        directory=r'mammo-calc\\analyzer_calc_data\\train',\n",
        "        batch_size=generator_batch_size,\n",
        "        target_size=(256, 256),\n",
        "        shuffle=True,\n",
        "        subset=subset,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "\n",
        "def create_test_flow(generator, directory):\n",
        "    return generator.flow_from_directory(\n",
        "        directory=directory, \n",
        "        batch_size=generator_batch_size,\n",
        "        target_size=(256, 256),\n",
        "        shuffle=False,\n",
        "        class_mode='binary'\n",
        "    )\n",
        "def create_testgen():\n",
        "    return tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "data_generator = create_datagen()\n",
        "test_gen = create_testgen()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScOCAwzFRR-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_generator = create_flow(data_generator, 'training')\n",
        "unlabeled_generator = create_flow(data_generator, 'validation')\n",
        "test_generator = create_test_flow(test_gen, r'mammo-calc\\analyzer_calc_data\\test')\n",
        "validation_generator = create_test_flow(test_gen, r'mammo-calc\\analyzer_calc_data\\validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hslkSNDKRXD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = generator_batch_size\n",
        "\n",
        "# https://github.com/google-research/mixmatch/blob/master/libml/layers.py\n",
        "def interleave_offsets(batch, nu):\n",
        "    groups = [batch // (nu + 1)] * (nu + 1)\n",
        "    for x in range(batch - sum(groups)):\n",
        "        groups[-x - 1] += 1\n",
        "    offsets = [0]\n",
        "    for g in groups:\n",
        "        offsets.append(offsets[-1] + g)\n",
        "    assert offsets[-1] == batch\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def interleave(xy, batch):\n",
        "    nu = len(xy) - 1\n",
        "    offsets = interleave_offsets(batch, nu)\n",
        "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
        "    for i in range(1, nu + 1):\n",
        "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
        "    return [tf.concat(v, axis=0) for v in xy]\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def sharpen(preds, temperature=0.5):\n",
        "    print(preds)\n",
        "    preds_target = tf.math.pow(preds, 1. / temperature)\n",
        "    preds_target /= tf.math.reduce_sum(preds_target, axis=1, keepdims=True)\n",
        "\n",
        "    return preds_target\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def augment_labeled(images):\n",
        "    images = tf.image.random_flip_left_right(images)\n",
        "    images = tf.image.random_flip_up_down(images)\n",
        "    images = tf.image.random_contrast(images, 0.8, 1.0)\n",
        "    images = tf.image.random_brightness(images, max_delta=0.2)\n",
        "    return tf.clip_by_value(images, 0.0, 1.0)\n",
        "\n",
        "\n",
        "# https://github.com/google-research/mixmatch/blob/master/mixmatch.py\n",
        "def mix_up(x_first, y_first, x_second, y_second, alpha=0.4):\n",
        "    assert x_first.shape[0] == x_second.shape[0], \"Array sizes differ.\"\n",
        "\n",
        "    mix = tf.compat.v1.distributions.Beta(alpha, alpha).sample([tf.shape(x_first)[0], 1, 1, 1])\n",
        "    mix = tf.maximum(mix, 1 - mix)\n",
        "\n",
        "    x_mix = mix * x_first + (1 - mix) * x_second\n",
        "    y_mix = mix[:, :, 0, 0] * y_first + (1 - mix[:, :, 0, 0]) * y_second\n",
        "\n",
        "    return x_mix, y_mix\n",
        "\n",
        "@tf.function\n",
        "def binary_sharpen(preds, temperature=0.5):\n",
        "    preds = tf.reshape(preds, [BATCH_SIZE,])\n",
        "    preds_target = (tf.nn.tanh((2/temperature)*(preds-0.5))+1)/2\n",
        "    preds_target = tf.reshape(preds_target, [BATCH_SIZE,1])\n",
        "    return preds_target\n",
        "\n",
        "def weight_decay(model, decay_rate):\n",
        "    for var in model.trainable_variables:\n",
        "        var.assign(var * (1 - decay_rate))\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def shuffle_tensors_together(tensor1, tensor2):\n",
        "    indices = tf.range(start=0, limit=tf.shape(tensor1)[0], dtype=tf.int32)\n",
        "    shuffled_indices = tf.random.shuffle(indices)\n",
        "\n",
        "    tensor1_shuffled = tf.gather(tensor1, shuffled_indices)\n",
        "    tensor2_shuffled = tf.gather(tensor2, shuffled_indices)\n",
        "\n",
        "    return tensor1_shuffled, tensor2_shuffled\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def random_crop(images):\n",
        "    padded_images = tf.pad(images, paddings=[(0, 0), (16, 16), (16, 16), (0, 0)], mode='REFLECT')\n",
        "    cropped_images = tf.map_fn(lambda image: tf.image.random_crop(image, size=(256, 256, 3)), padded_images)\n",
        "    return cropped_images\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def get_losses(labeled_logits, labeled_targets, unlabeled_logits, unlabeled_targets):\n",
        "    labeled_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labeled_targets, logits=labeled_logits)\n",
        "    labeled_loss = tf.reduce_mean(labeled_loss)\n",
        "\n",
        "    unlabeled_loss = tf.square(unlabeled_targets - tf.nn.sigmoid(unlabeled_logits))\n",
        "    unlabeled_loss = tf.reduce_mean(unlabeled_loss)\n",
        "\n",
        "    return labeled_loss, unlabeled_loss\n",
        "\n",
        "\n",
        "def mixmatch(model, x_augmented_labeled_batch, y_labeled_batch_train, x_unlabeled_batch, unlabeled_augments,\n",
        "             batch_size=BATCH_SIZE):\n",
        "    logits = []\n",
        "\n",
        "    for aug in unlabeled_augments:\n",
        "        if aug == 'udflip':\n",
        "            augmented_unlabeled_batch_ud = tf.image.random_flip_up_down(x_unlabeled_batch)\n",
        "            ud_guessed_label = tf.nn.sigmoid(model(augmented_unlabeled_batch_ud))\n",
        "            logits.append(binary_sharpen(ud_guessed_label))\n",
        "        if aug == 'lrflip':\n",
        "            augmented_unlabeled_batch_lr = tf.image.flip_left_right(x_unlabeled_batch)\n",
        "            lr_guessed_label = tf.nn.sigmoid(model(augmented_unlabeled_batch_lr))\n",
        "            logits.append(binary_sharpen(lr_guessed_label))\n",
        "        if aug == 'brightness':\n",
        "            augmented_unlabeled_batch_br = tf.clip_by_value(tf.image.random_brightness(x_unlabeled_batch, max_delta=0.2), 0.0, 1.0)\n",
        "            br_guessed_label = tf.nn.sigmoid(model(augmented_unlabeled_batch_br))\n",
        "            logits.append(binary_sharpen(br_guessed_label))\n",
        "\n",
        "    logits = tf.add_n(logits) / len(unlabeled_augments)\n",
        "    guessed_labels = logits #binary_sharpen(logits)\n",
        "    guessed_labels = tf.stop_gradient(guessed_labels)\n",
        "\n",
        "    x_unlabeled_all_augmentations = tf.concat([augmented_unlabeled_batch_ud, augmented_unlabeled_batch_lr, augmented_unlabeled_batch_br],\n",
        "                                              axis=0)\n",
        "    y_unlabeled_all_augmentations = tf.concat([guessed_labels, guessed_labels, guessed_labels], axis=0)\n",
        "\n",
        "    x_combined_batch = tf.concat([x_augmented_labeled_batch, x_unlabeled_all_augmentations], axis=0)\n",
        "    y_combined_batch = tf.concat([y_labeled_batch_train, y_unlabeled_all_augmentations], axis=0)\n",
        "\n",
        "    shuffled_x, shuffled_y = shuffle_tensors_together(x_combined_batch, y_combined_batch)\n",
        "    del x_unlabeled_all_augmentations, y_unlabeled_all_augmentations, augmented_unlabeled_batch_ud, \\\n",
        "            augmented_unlabeled_batch_lr, augmented_unlabeled_batch_br\n",
        "    x_combined_mix, y_combined_mix = mix_up(x_combined_batch, y_combined_batch, shuffled_x, shuffled_y, alpha=0.75)\n",
        "    x_combined_mix = tf.split(x_combined_mix, len(unlabeled_augments) + 1, axis=0)\n",
        "    x_combined_mix_interleaved = interleave(x_combined_mix, batch_size)\n",
        "\n",
        "    return x_combined_mix_interleaved, y_combined_mix\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seCn1hTsRvsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Train with MixMatch\n",
        "print(tf.__version__)\n",
        "max_vall_acc = None\n",
        "batch_size = BATCH_SIZE\n",
        "model = WideResNet(1, depth=16, width=2)\n",
        "model.build(input_shape=(None, 256, 256, 3))\n",
        "\n",
        "ramp_up_length = 10000\n",
        "w_decay = 0.00001\n",
        "unlabeled_augments = ['udflip', 'lrflip', 'brightness']\n",
        "unlabeled_weight = 30\n",
        "\n",
        "\n",
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "\n",
        "avg_train_loss = tf.keras.metrics.Mean()\n",
        "avg_labeled_loss = tf.keras.metrics.Mean()\n",
        "avg_unlabeled_loss = tf.keras.metrics.Mean()\n",
        "avg_val_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "# Prepare the training dataset.\n",
        "\n",
        "labeled_iter = iter(train_generator)\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "# Iterate over epochs.\n",
        "for epoch in range(20):\n",
        "    epoch_start = tf.timestamp(name=None)\n",
        "    labeled_step = 1\n",
        "    valid_step = 1\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for x_unlabeled_batch, _ in unlabeled_generator:\n",
        "        if(x_unlabeled_batch.shape[0] != BATCH_SIZE):\n",
        "            x_unlabeled_batch, _ = next(unlabeled_generator)\n",
        "        if labeled_step%(2046//BATCH_SIZE)==0:\n",
        "            break\n",
        "\n",
        "        x_labeled_batch_train, y_labeled_batch_train = next(labeled_iter)\n",
        "        if(x_labeled_batch_train.shape[0] != BATCH_SIZE):\n",
        "            x_labeled_batch_train, y_labeled_batch_train = next(labeled_iter)\n",
        "            \n",
        "        y_labeled_batch_train = np.expand_dims(y_labeled_batch_train, axis=1)\n",
        "\n",
        "        x_augmented_labeled_batch = augment_labeled(x_labeled_batch_train)\n",
        "\n",
        "        x_combined_mix_interleaved, y_combined_mix = mixmatch(model, x_augmented_labeled_batch,\n",
        "                                                              y_labeled_batch_train,\n",
        "                                                              x_unlabeled_batch,\n",
        "                                                              unlabeled_augments,\n",
        "                                                             BATCH_SIZE)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            train_logits_interleaved = [model(x_combined_mix_interleaved[0])]\n",
        "            for batch in x_combined_mix_interleaved[1:]:\n",
        "                train_logits_interleaved.append(model(batch))\n",
        "\n",
        "            train_logits = interleave(train_logits_interleaved, batch_size)\n",
        "            labeled_logits = train_logits[0]\n",
        "            unlabeled_logits = tf.concat(train_logits[1:], axis=0)\n",
        "\n",
        "            labeled_loss, unlabeled_loss = get_losses(labeled_logits, y_combined_mix[:batch_size],\n",
        "                                                      unlabeled_logits,\n",
        "                                                      y_combined_mix[batch_size:])\n",
        "            avg_unlabeled_loss(unlabeled_loss)\n",
        "            avg_labeled_loss(labeled_loss)\n",
        "\n",
        "            ramp_up = 1.0 if global_step > ramp_up_length else global_step / ramp_up_length\n",
        "            combined_loss = labeled_loss + ramp_up * unlabeled_weight * unlabeled_loss\n",
        "\n",
        "        grads = tape.gradient(combined_loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        global_step = global_step + 1\n",
        "        weight_decay(model=model, decay_rate=w_decay)\n",
        "        avg_train_loss(combined_loss)\n",
        "        # Update training metric.\n",
        "        train_acc_metric(y_labeled_batch_train, tf.nn.sigmoid(model(x_labeled_batch_train)))\n",
        "        del x_combined_mix_interleaved, y_combined_mix\n",
        "        labeled_step += 1\n",
        "\n",
        "    for x_batch_val, y_batch_val in validation_generator:\n",
        "        if valid_step%(604//BATCH_SIZE) == 0:\n",
        "            break\n",
        "        if(x_batch_val.shape[0] != BATCH_SIZE):\n",
        "                x_batch_val, y_batch_val = next(validation_generator)\n",
        "        val_logits = model(x_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        val_acc_metric(y_batch_val, tf.nn.sigmoid(val_logits))\n",
        "        val_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_batch_val, logits=tf.squeeze(val_logits, axis=1))\n",
        "        avg_val_loss(tf.reduce_mean(val_loss))\n",
        "        valid_step = valid_step + 1\n",
        "    epoch_end = tf.timestamp(name=None)\n",
        "    print(f'Epoch: {epoch:4d}, Train loss: {avg_train_loss.result():.4f}, Train Acc: {train_acc_metric.result():.3%},'\n",
        "            f' Val loss: {avg_val_loss.result():.4f}, Val Acc: {val_acc_metric.result():.3%},'\n",
        "            f' Lambda: {float(ramp_up * unlabeled_weight):.4f}, Total steps: {global_step}',)\n",
        "    print(f'Labeled loss: {avg_labeled_loss.result():.4f}, Unlabeled loss: {avg_unlabeled_loss.result():.4f}, '\n",
        "                f'Time: {str(int((epoch_end.numpy() - epoch_start.numpy())))}s')\n",
        "    avg_train_loss.reset_states()\n",
        "        \n",
        "\n",
        "    if max_val_acc is None:\n",
        "        max_val_acc = val_acc_metric.result()\n",
        "        model.save_weights(f'model_ {epoch}_{max_val_acc}.h5')\n",
        "    elif val_acc_metric.result() > max_val_acc:\n",
        "        max_val_acc = val_acc_metric.result()\n",
        "        model.save_weights(f'model_ {epoch}_{max_val_acc}.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}