{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of CIFAR-10 MixMatch",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzhPEZdsI-Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow_probability as tfp "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov0z150TLWH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#https://raw.githubusercontent.com/ntozer/mixmatch-tensorflow2.0/master/model.py\n",
        "\n",
        "class Residual3x3Unit(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels_in, channels_out, stride, droprate=0., activate_before_residual=False):\n",
        "        super(Residual3x3Unit, self).__init__()\n",
        "        self.bn_0 = tf.keras.layers.BatchNormalization(momentum=0.999)\n",
        "        self.relu_0 = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "        self.conv_0 = tf.keras.layers.Conv2D(channels_out, kernel_size=3, strides=stride, padding='same', use_bias=False)\n",
        "        self.bn_1 = tf.keras.layers.BatchNormalization(momentum=0.999)\n",
        "        self.relu_1 = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "        self.conv_1 = tf.keras.layers.Conv2D(channels_out, kernel_size=3, strides=1, padding='same', use_bias=False)\n",
        "        self.downsample = channels_in != channels_out\n",
        "        self.shortcut = tf.keras.layers.Conv2D(channels_out, kernel_size=1, strides=stride, use_bias=False)\n",
        "        self.activate_before_residual = activate_before_residual\n",
        "        self.dropout = tf.keras.layers.Dropout(rate=droprate)\n",
        "        self.droprate = droprate\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=True):\n",
        "        if self.downsample and self.activate_before_residual:\n",
        "            x = self.relu_0(self.bn_0(x, training=training))\n",
        "        elif not self.downsample:\n",
        "            out = self.relu_0(self.bn_0(x, training=training))\n",
        "        out = self.relu_1(self.bn_1(self.conv_0(x if self.downsample else out), training=training))\n",
        "        if self.droprate > 0.:\n",
        "            out = self.dropout(out)\n",
        "        out = self.conv_1(out)\n",
        "        return out + (self.shortcut(x) if self.downsample else x)\n",
        "\n",
        "\n",
        "class ResidualBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_units, channels_in, channels_out, unit, stride, droprate=0., activate_before_residual=False):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.units = self._build_unit(n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual)\n",
        "\n",
        "    def _build_unit(self, n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual):\n",
        "        units = []\n",
        "        for i in range(n_units):\n",
        "            units.append(unit(channels_in if i == 0 else channels_out, channels_out, stride if i == 0 else 1, droprate, activate_before_residual))\n",
        "        return units\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, x, training=True):\n",
        "        for unit in self.units:\n",
        "            x = unit(x, training=training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class WideResNet(tf.keras.Model):\n",
        "    def __init__(self, num_classes, depth=28, width=2, droprate=0., input_shape=(None, 32, 32, 3), **kwargs):\n",
        "        super(WideResNet, self).__init__(input_shape, **kwargs)\n",
        "        assert (depth - 4) % 6 == 0\n",
        "        N = int((depth - 4) / 6)\n",
        "        channels = [16, 16 * width, 32 * width, 64 * width]\n",
        "\n",
        "        self.conv_0 = tf.keras.layers.Conv2D(channels[0], kernel_size=3, strides=1, padding='same', use_bias=False)\n",
        "        self.block_0 = ResidualBlock(N, channels[0], channels[1], Residual3x3Unit, 1, droprate, True)\n",
        "        self.block_1 = ResidualBlock(N, channels[1], channels[2], Residual3x3Unit, 2, droprate)\n",
        "        self.block_2 = ResidualBlock(N, channels[2], channels[3], Residual3x3Unit, 2, droprate)\n",
        "        self.bn_0 = tf.keras.layers.BatchNormalization(momentum=0.999)\n",
        "        self.relu_0 = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
        "        self.avg_pool = tf.keras.layers.AveragePooling2D((8, 8), (1, 1))\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=True):\n",
        "        x = inputs\n",
        "        x = self.conv_0(x)\n",
        "        x = self.block_0(x, training=training)\n",
        "        x = self.block_1(x, training=training)\n",
        "        x = self.block_2(x, training=training)\n",
        "        x = self.relu_0(self.bn_0(x, training=training))\n",
        "        x = self.avg_pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PalxqUpQLkmh",
        "colab_type": "code",
        "outputId": "7992fafa-3e86-4ada-f7e9-ff175bc97481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "n_classes = 10\n",
        "\n",
        "x_test = x_test.astype('float32')\n",
        "x_test /= 255.\n",
        "y_test = tf.keras.utils.to_categorical(y_test, n_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7bk_XGJLmJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training_validation_set_split(x_train, y_train, labeled_samples_per_class, fix_validation_size = True, validation_split = 0.2, shuffle = True, n_classes = 10):\n",
        "    \n",
        "    assert len(x_train) == len(y_train), \"x_train and y_train lengths do not match.\"\n",
        "    \n",
        "    labeled_training_idx = []\n",
        "    labeled_validation_idx = []\n",
        "    unlabeled_training_idx = []\n",
        "    \n",
        "    if(fix_validation_size is True):\n",
        "        n_train_per_class = labeled_samples_per_class\n",
        "        n_val_per_class = 500\n",
        "    else:\n",
        "        n_val_per_class = int(labeled_samples_per_class * validation_split)\n",
        "        n_train_per_class =  labeled_samples_per_class - n_val_per_class    \n",
        "\n",
        "    for class_idx in range(n_classes):\n",
        "        indices = np.where(y_train == class_idx)[0]\n",
        "        if(shuffle == True):\n",
        "            np.random.shuffle(indices)\n",
        "        \n",
        "        labeled_training_idx.extend(indices[:n_train_per_class])\n",
        "        labeled_validation_idx.extend(indices[-n_val_per_class:])\n",
        "        unlabeled_training_idx.extend(indices[n_train_per_class:-n_val_per_class])\n",
        "\n",
        "    np.random.shuffle(labeled_training_idx)\n",
        "    np.random.shuffle(labeled_validation_idx)\n",
        "    np.random.shuffle(unlabeled_training_idx)\n",
        "   \n",
        "    x_labeled_training = x_train[labeled_training_idx]\n",
        "    y_labeled_training = y_train[labeled_training_idx]\n",
        "    \n",
        "    x_labeled_validation = x_train[labeled_validation_idx]\n",
        "    y_labeled_validation = y_train[labeled_validation_idx]\n",
        "    \n",
        "    x_unlabeled_training = x_train[unlabeled_training_idx]\n",
        "    \n",
        "    return x_labeled_training, y_labeled_training, x_labeled_validation, y_labeled_validation, x_unlabeled_training"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCCaA-gNLn-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 2019\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "x_labeled_training, y_labeled_training, x_labeled_validation, y_labeled_validation, x_unlabeled_training = \\\n",
        "    training_validation_set_split(x_train,y_train,labeled_samples_per_class=25, fix_validation_size=True,shuffle=True)\n",
        "x_labeled_training = x_labeled_training.astype('float32')\n",
        "x_labeled_validation = x_labeled_validation.astype('float32')\n",
        "x_labeled_training /= 255.\n",
        "x_labeled_validation /= 255.\n",
        "x_unlabeled_training = x_unlabeled_training.astype('float32')\n",
        "x_unlabeled_training /= 255.\n",
        "y_labeled_training = tf.keras.utils.to_categorical(y_labeled_training, n_classes)\n",
        "y_labeled_validation = tf.keras.utils.to_categorical(y_labeled_validation, n_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gEsAIASLpP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def sharpen(preds, temperature = 0.5):\n",
        "    preds_target = tf.math.pow(preds, 1. / temperature)\n",
        "    preds_target /= tf.math.reduce_sum(preds_target, axis=1, keepdims=True)\n",
        "    \n",
        "    return preds_target\n",
        "\n",
        "@tf.function\n",
        "def augment_labeled(images):\n",
        "    images = tf.image.random_flip_left_right(images)\n",
        "    images = tf.image.random_contrast(images, 0.8, 1.0)\n",
        "    images = tf.image.random_hue(images,max_delta = 0.2)\n",
        "    images = tf.image.random_brightness(images,max_delta = 0.2)\n",
        "    choice = tf.random.uniform(shape=[], minval=0., maxval=1., dtype=tf.float32)\n",
        "    if(choice > 0.5):\n",
        "        images = tf.pad(images, paddings=[(0, 0), (4, 4), (4, 4), (0, 0)], mode='REFLECT')\n",
        "        images = tf.map_fn(lambda image: tf.image.random_crop(image, size=(32, 32, 3)), images)\n",
        "    return tf.clip_by_value(images,0.0,1.0)\n",
        "\n",
        "#https://github.com/google-research/mixmatch/blob/master/mixmatch.py\n",
        "def mix_up(x_first, y_first, x_second, y_second, alpha = 0.75):\n",
        "    assert x_first.shape[0] == x_second.shape[0], \"Array sizes differ.\"\n",
        "    \n",
        "    mix = tfp.distributions.Beta(alpha, alpha).sample([tf.shape(x_first)[0], 1, 1, 1])\n",
        "    mix = tf.maximum(mix, 1 - mix)\n",
        "    \n",
        "    x_mix = mix * x_first + (1-mix) * x_second\n",
        "    y_mix = mix[:, :, 0, 0] * y_first + (1-mix[:, :, 0, 0]) * y_second\n",
        "    \n",
        "    return x_mix, y_mix\n",
        "\n",
        "def weight_decay(model, decay_rate):\n",
        "    for var in model.trainable_variables:\n",
        "        var.assign(var * (1 - decay_rate))\n",
        "        \n",
        "@tf.function\n",
        "def shuffle_tensors_together(tensor1, tensor2):    \n",
        "    indices = tf.range(start=0, limit=tf.shape(tensor1)[0], dtype=tf.int32)\n",
        "    shuffled_indices = tf.random.shuffle(indices)\n",
        "\n",
        "    tensor1_shuffled = tf.gather(tensor1, shuffled_indices)\n",
        "    tensor2_shuffled = tf.gather(tensor2, shuffled_indices)\n",
        "    \n",
        "    return tensor1_shuffled, tensor2_shuffled\n",
        "\n",
        "@tf.function\n",
        "def random_crop(images):\n",
        "    padded_images = tf.pad(images, paddings=[(0, 0), (4, 4), (4, 4), (0, 0)], mode='REFLECT')\n",
        "    cropped_images = tf.map_fn(lambda image: tf.image.random_crop(image, size=(32, 32, 3)), padded_images)\n",
        "    return cropped_images\n",
        "\n",
        "@tf.function\n",
        "def get_losses(labeled_logits, labeled_targets, unlabeled_logits, unlabeled_targets):\n",
        "    labeled_loss = tf.nn.softmax_cross_entropy_with_logits(labels=labeled_targets, logits=labeled_logits)\n",
        "    labeled_loss = tf.reduce_mean(labeled_loss)\n",
        "    \n",
        "    unlabeled_loss = tf.square(unlabeled_targets - tf.nn.softmax(unlabeled_logits))\n",
        "    unlabeled_loss = tf.reduce_mean(unlabeled_loss)\n",
        "\n",
        "    return labeled_loss, unlabeled_loss\n",
        "\n",
        "def interleave_offsets(batch, nu):\n",
        "    groups = [batch // (nu + 1)] * (nu + 1)\n",
        "    for x in range(batch - sum(groups)):\n",
        "        groups[-x - 1] += 1\n",
        "    offsets = [0]\n",
        "    for g in groups:\n",
        "        offsets.append(offsets[-1] + g)\n",
        "    assert offsets[-1] == batch\n",
        "    return offsets\n",
        "\n",
        "\n",
        "def interleave(xy, batch):\n",
        "    nu = len(xy) - 1\n",
        "    offsets = interleave_offsets(batch, nu)\n",
        "    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n",
        "    for i in range(1, nu + 1):\n",
        "        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n",
        "    return [tf.concat(v, axis=0) for v in xy]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xF-5PWpaLqkR",
        "colab_type": "code",
        "outputId": "1192cf6e-be5b-4111-ebae-d79464eba9de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "from keras_radam.training import RAdamOptimizer\n",
        "\n",
        "ramp_up_length = 6000\n",
        "w_decay = 0.000035\n",
        "unlabeled_augments = ['random_crop','lrflip']\n",
        "unlabeled_weight = 75\n",
        "\n",
        "# Get the model.\n",
        "model = WideResNet(n_classes, depth=28, width=2)\n",
        "model.build(input_shape=(None, 32, 32, 3))\n",
        "\n",
        "initial_learning_rate = 0.002\n",
        "\n",
        "# Instantiate an optimizer to train the model.\n",
        "optimizer = RAdamOptimizer(learning_rate=initial_learning_rate)\n",
        "\n",
        "# Prepare the metrics.\n",
        "train_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "avg_train_loss = tf.keras.metrics.Mean()\n",
        "avg_labeled_loss = tf.keras.metrics.Mean()\n",
        "avg_unlabeled_loss =  tf.keras.metrics.Mean()\n",
        "avg_val_loss = tf.keras.metrics.Mean()\n",
        "\n",
        "# Prepare the training dataset.\n",
        "batch_size = 64\n",
        "labeled_train_dataset = tf.data.Dataset.from_tensor_slices((x_labeled_training, y_labeled_training))\n",
        "labeled_train_dataset = labeled_train_dataset.batch(batch_size, drop_remainder=True)\n",
        "labeled_iter = iter(labeled_train_dataset)\n",
        "\n",
        "unlabeled_train_dataset = tf.data.Dataset.from_tensor_slices(x_unlabeled_training)\n",
        "unlabeled_train_dataset = unlabeled_train_dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_labeled_validation, y_labeled_validation))\n",
        "val_dataset = val_dataset.batch(64)\n",
        "\n",
        "global_step = 0\n",
        "\n",
        "# Iterate over epochs.\n",
        "for epoch in range(400):\n",
        "    epoch_start = tf.timestamp(name=None)\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for x_unlabeled_batch in unlabeled_train_dataset:\n",
        "        try: \n",
        "            x_labeled_batch_train, y_labeled_batch_train = next(labeled_iter)\n",
        "        except:\n",
        "            labeled_iter = iter(labeled_train_dataset)\n",
        "            x_labeled_batch_train, y_labeled_batch_train = next(labeled_iter)\n",
        "\n",
        "        # Augment labeled\n",
        "        x_augmented_labeled_batch = augment_labeled(x_labeled_batch_train)\n",
        "\n",
        "        # Unlabeled predictions\n",
        "        logits = []\n",
        "        \n",
        "        for aug in unlabeled_augments:\n",
        "            if(aug == 'random_crop'):\n",
        "                augmented_unlabeled_batch_crop = random_crop(x_unlabeled_batch)\n",
        "                crop_guessed_label = tf.nn.softmax(model(augmented_unlabeled_batch_crop))\n",
        "                logits.append(crop_guessed_label)\n",
        "            if(aug == 'lrflip'):\n",
        "                augmented_unlabeled_batch_lr = tf.image.flip_left_right(x_unlabeled_batch)\n",
        "                lr_guessed_label = tf.nn.softmax(model(augmented_unlabeled_batch_lr))\n",
        "                logits.append(lr_guessed_label)\n",
        "\n",
        "        # Aggregate unlabeled predictions and sharpen        \n",
        "        logits = tf.add_n(logits) / len(unlabeled_augments)\n",
        "        guessed_labels = sharpen(logits)\n",
        "        guessed_labels = tf.stop_gradient(guessed_labels)\n",
        "\n",
        "        # Prepare W\n",
        "        x_unlabeled_all_augmentations = tf.concat([augmented_unlabeled_batch_crop, augmented_unlabeled_batch_lr], axis=0)\n",
        "        y_unlabeled_all_augmentations = tf.concat([guessed_labels, guessed_labels], axis=0)\n",
        "        \n",
        "        x_combined_batch = tf.concat([x_augmented_labeled_batch, x_unlabeled_all_augmentations], axis=0)\n",
        "        y_combined_batch = tf.concat([y_labeled_batch_train, y_unlabeled_all_augmentations], axis=0)\n",
        "\n",
        "        shuffled_x, shuffled_y = shuffle_tensors_together(x_combined_batch,y_combined_batch)\n",
        "        \n",
        "        # MixUp on X+U, W\n",
        "        XU, XUy = mix_up(x_combined_batch, y_combined_batch, \\\n",
        "                             shuffled_x, shuffled_y, alpha = 0.75)\n",
        "        \n",
        "        # Split to batches\n",
        "        XU = tf.split(XU, len(unlabeled_augments) + 1, axis=0)\n",
        "\n",
        "        # Mix samples within batches \n",
        "        # https://github.com/google-research/mixmatch/issues/5\n",
        "        XU = interleave(XU, batch_size)\n",
        "        \n",
        "        # Predict\n",
        "        with tf.GradientTape() as tape:\n",
        "            train_logits = [model(XU[0])]\n",
        "            for batch in XU[1:]:\n",
        "                train_logits.append(model(batch))\n",
        "\n",
        "            # Restore original batches\n",
        "            train_logits = interleave(train_logits, batch_size)\n",
        "\n",
        "            # Calculate loss\n",
        "            labeled_logits = train_logits[0]\n",
        "            unlabeled_logits = tf.concat(train_logits[1:], axis=0)\n",
        "\n",
        "            labeled_loss, unlabeled_loss = get_losses(labeled_logits,XUy[:batch_size],unlabeled_logits,XUy[batch_size:])\n",
        "\n",
        "            avg_unlabeled_loss(unlabeled_loss)\n",
        "            avg_labeled_loss(labeled_loss)\n",
        "\n",
        "            ramp_up = 1.0 if global_step > ramp_up_length else global_step / ramp_up_length\n",
        "            combined_loss = labeled_loss + ramp_up * unlabeled_weight * unlabeled_loss\n",
        "\n",
        "        # Calculate gradients, update metrics    \n",
        "        grads = tape.gradient(combined_loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        global_step = global_step + 1\n",
        "        weight_decay(model = model, decay_rate = w_decay)\n",
        "        avg_train_loss(combined_loss)\n",
        "\n",
        "\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_logits = model(x_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        val_acc_metric(y_batch_val, tf.nn.softmax(val_logits))\n",
        "        val_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y_batch_val, logits=val_logits)\n",
        "        avg_val_loss(tf.reduce_mean(val_loss))\n",
        "        \n",
        "    epoch_end = tf.timestamp(name=None)\n",
        "    print(f'Epoch: {epoch:4d} , Train loss: {avg_train_loss.result():.4f}, Val loss: {avg_val_loss.result():.4f}, Val Acc: {val_acc_metric.result():.3%}, Lambda: {float(ramp_up * unlabeled_weight):.4f}' )\n",
        "    print(f'Labeled loss: {avg_labeled_loss.result():.4f} , Unlabeled loss: {avg_unlabeled_loss.result():.4f}, LR: {learning_rate_fn(global_step).numpy():2f}, Time:: {str(int((epoch_end.numpy()-epoch_start.numpy())))}s')\n",
        "    \n",
        "    train_acc_metric.reset_states()    \n",
        "    val_acc_metric.reset_states()\n",
        "    avg_train_loss.reset_states()\n",
        "    avg_val_loss.reset_states()\n",
        "    avg_labeled_loss.reset_states()\n",
        "    avg_unlabeled_loss.reset_states()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 , Train loss: 1.2989, Val loss: 3.4729, Val Acc: 23.760%, Lambda: 8.7250\n",
            "Labeled loss: 1.2478 , Unlabeled loss: 0.0110, LR: 0.001931, Time:: 604s\n",
            "Epoch:    1 , Train loss: 1.0707, Val loss: 2.7451, Val Acc: 39.980%, Lambda: 17.4625\n",
            "Labeled loss: 0.9307 , Unlabeled loss: 0.0107, LR: 0.001865, Time:: 596s\n",
            "Epoch:    2 , Train loss: 1.1106, Val loss: 4.3641, Val Acc: 30.860%, Lambda: 26.2000\n",
            "Labeled loss: 0.8928 , Unlabeled loss: 0.0100, LR: 0.001801, Time:: 594s\n",
            "Epoch:    3 , Train loss: 1.1704, Val loss: 2.2529, Val Acc: 48.860%, Lambda: 34.9375\n",
            "Labeled loss: 0.8765 , Unlabeled loss: 0.0096, LR: 0.001739, Time:: 595s\n",
            "Epoch:    4 , Train loss: 1.2271, Val loss: 2.3782, Val Acc: 47.880%, Lambda: 43.6750\n",
            "Labeled loss: 0.8600 , Unlabeled loss: 0.0093, LR: 0.001679, Time:: 593s\n",
            "Epoch:    5 , Train loss: 1.2909, Val loss: 2.2606, Val Acc: 45.680%, Lambda: 52.4125\n",
            "Labeled loss: 0.8533 , Unlabeled loss: 0.0091, LR: 0.001622, Time:: 593s\n",
            "Epoch:    6 , Train loss: 1.3567, Val loss: 1.8888, Val Acc: 55.680%, Lambda: 61.1500\n",
            "Labeled loss: 0.8438 , Unlabeled loss: 0.0090, LR: 0.001566, Time:: 591s\n",
            "Epoch:    7 , Train loss: 1.4271, Val loss: 1.9461, Val Acc: 54.340%, Lambda: 69.8875\n",
            "Labeled loss: 0.8467 , Unlabeled loss: 0.0089, LR: 0.001512, Time:: 591s\n",
            "Epoch:    8 , Train loss: 1.4863, Val loss: 1.8202, Val Acc: 54.460%, Lambda: 75.0000\n",
            "Labeled loss: 0.8371 , Unlabeled loss: 0.0088, LR: 0.001460, Time:: 591s\n",
            "Epoch:    9 , Train loss: 1.4788, Val loss: 2.0340, Val Acc: 51.340%, Lambda: 75.0000\n",
            "Labeled loss: 0.8229 , Unlabeled loss: 0.0087, LR: 0.001410, Time:: 591s\n",
            "Epoch:   10 , Train loss: 1.4650, Val loss: 1.6839, Val Acc: 56.660%, Lambda: 75.0000\n",
            "Labeled loss: 0.8165 , Unlabeled loss: 0.0086, LR: 0.001362, Time:: 591s\n",
            "Epoch:   11 , Train loss: 1.4450, Val loss: 1.4331, Val Acc: 59.760%, Lambda: 75.0000\n",
            "Labeled loss: 0.8054 , Unlabeled loss: 0.0085, LR: 0.001315, Time:: 592s\n",
            "Epoch:   12 , Train loss: 1.4529, Val loss: 1.8831, Val Acc: 56.660%, Lambda: 75.0000\n",
            "Labeled loss: 0.8073 , Unlabeled loss: 0.0086, LR: 0.001270, Time:: 592s\n",
            "Epoch:   13 , Train loss: 1.4460, Val loss: 1.5151, Val Acc: 60.400%, Lambda: 75.0000\n",
            "Labeled loss: 0.8045 , Unlabeled loss: 0.0086, LR: 0.001226, Time:: 590s\n",
            "Epoch:   14 , Train loss: 1.4493, Val loss: 2.1799, Val Acc: 53.580%, Lambda: 75.0000\n",
            "Labeled loss: 0.8087 , Unlabeled loss: 0.0085, LR: 0.001184, Time:: 589s\n",
            "Epoch:   15 , Train loss: 1.4344, Val loss: 1.8123, Val Acc: 54.120%, Lambda: 75.0000\n",
            "Labeled loss: 0.7955 , Unlabeled loss: 0.0085, LR: 0.001143, Time:: 591s\n",
            "Epoch:   16 , Train loss: 1.4275, Val loss: 2.1583, Val Acc: 52.320%, Lambda: 75.0000\n",
            "Labeled loss: 0.7911 , Unlabeled loss: 0.0085, LR: 0.001104, Time:: 592s\n",
            "Epoch:   17 , Train loss: 1.4224, Val loss: 1.8630, Val Acc: 52.760%, Lambda: 75.0000\n",
            "Labeled loss: 0.7885 , Unlabeled loss: 0.0085, LR: 0.001066, Time:: 591s\n",
            "Epoch:   18 , Train loss: 1.4233, Val loss: 1.7354, Val Acc: 58.480%, Lambda: 75.0000\n",
            "Labeled loss: 0.7876 , Unlabeled loss: 0.0085, LR: 0.001029, Time:: 592s\n",
            "Epoch:   19 , Train loss: 1.4202, Val loss: 1.5991, Val Acc: 59.740%, Lambda: 75.0000\n",
            "Labeled loss: 0.7864 , Unlabeled loss: 0.0085, LR: 0.000994, Time:: 594s\n",
            "Epoch:   20 , Train loss: 1.4188, Val loss: 1.5810, Val Acc: 62.440%, Lambda: 75.0000\n",
            "Labeled loss: 0.7865 , Unlabeled loss: 0.0084, LR: 0.000960, Time:: 593s\n",
            "Epoch:   21 , Train loss: 1.4062, Val loss: 1.5313, Val Acc: 61.980%, Lambda: 75.0000\n",
            "Labeled loss: 0.7784 , Unlabeled loss: 0.0084, LR: 0.000927, Time:: 592s\n",
            "Epoch:   22 , Train loss: 1.4005, Val loss: 1.5659, Val Acc: 59.640%, Lambda: 75.0000\n",
            "Labeled loss: 0.7718 , Unlabeled loss: 0.0084, LR: 0.000895, Time:: 590s\n",
            "Epoch:   23 , Train loss: 1.3988, Val loss: 1.3759, Val Acc: 60.780%, Lambda: 75.0000\n",
            "Labeled loss: 0.7743 , Unlabeled loss: 0.0083, LR: 0.000864, Time:: 591s\n",
            "Epoch:   24 , Train loss: 1.3944, Val loss: 1.5315, Val Acc: 60.700%, Lambda: 75.0000\n",
            "Labeled loss: 0.7687 , Unlabeled loss: 0.0083, LR: 0.000835, Time:: 591s\n",
            "Epoch:   25 , Train loss: 1.3913, Val loss: 1.6103, Val Acc: 61.160%, Lambda: 75.0000\n",
            "Labeled loss: 0.7647 , Unlabeled loss: 0.0084, LR: 0.000806, Time:: 590s\n",
            "Epoch:   26 , Train loss: 1.3904, Val loss: 1.3492, Val Acc: 64.860%, Lambda: 75.0000\n",
            "Labeled loss: 0.7683 , Unlabeled loss: 0.0083, LR: 0.000778, Time:: 593s\n",
            "Epoch:   27 , Train loss: 1.3739, Val loss: 1.4142, Val Acc: 63.080%, Lambda: 75.0000\n",
            "Labeled loss: 0.7582 , Unlabeled loss: 0.0082, LR: 0.000752, Time:: 593s\n",
            "Epoch:   28 , Train loss: 1.3802, Val loss: 1.5989, Val Acc: 60.280%, Lambda: 75.0000\n",
            "Labeled loss: 0.7632 , Unlabeled loss: 0.0082, LR: 0.000726, Time:: 595s\n",
            "Epoch:   29 , Train loss: 1.3673, Val loss: 1.5713, Val Acc: 63.060%, Lambda: 75.0000\n",
            "Labeled loss: 0.7559 , Unlabeled loss: 0.0082, LR: 0.000701, Time:: 593s\n",
            "Epoch:   30 , Train loss: 1.3714, Val loss: 1.5743, Val Acc: 61.620%, Lambda: 75.0000\n",
            "Labeled loss: 0.7588 , Unlabeled loss: 0.0082, LR: 0.000677, Time:: 592s\n",
            "Epoch:   31 , Train loss: 1.3663, Val loss: 1.5408, Val Acc: 62.620%, Lambda: 75.0000\n",
            "Labeled loss: 0.7550 , Unlabeled loss: 0.0082, LR: 0.000653, Time:: 592s\n",
            "Epoch:   32 , Train loss: 1.3677, Val loss: 1.4453, Val Acc: 64.800%, Lambda: 75.0000\n",
            "Labeled loss: 0.7546 , Unlabeled loss: 0.0082, LR: 0.000631, Time:: 591s\n",
            "Epoch:   33 , Train loss: 1.3640, Val loss: 1.3769, Val Acc: 63.240%, Lambda: 75.0000\n",
            "Labeled loss: 0.7517 , Unlabeled loss: 0.0082, LR: 0.000609, Time:: 592s\n",
            "Epoch:   34 , Train loss: 1.3566, Val loss: 1.6668, Val Acc: 63.340%, Lambda: 75.0000\n",
            "Labeled loss: 0.7504 , Unlabeled loss: 0.0081, LR: 0.000588, Time:: 592s\n",
            "Epoch:   35 , Train loss: 1.3536, Val loss: 1.5363, Val Acc: 65.320%, Lambda: 75.0000\n",
            "Labeled loss: 0.7497 , Unlabeled loss: 0.0081, LR: 0.000568, Time:: 590s\n",
            "Epoch:   36 , Train loss: 1.3503, Val loss: 1.4048, Val Acc: 65.780%, Lambda: 75.0000\n",
            "Labeled loss: 0.7449 , Unlabeled loss: 0.0081, LR: 0.000549, Time:: 588s\n",
            "Epoch:   37 , Train loss: 1.3508, Val loss: 1.4449, Val Acc: 63.340%, Lambda: 75.0000\n",
            "Labeled loss: 0.7452 , Unlabeled loss: 0.0081, LR: 0.000530, Time:: 590s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbetlGPwxn3H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}