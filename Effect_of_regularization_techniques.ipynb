{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Effect of regularization techniques",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkzWiL2fMD6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import cifar10\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Flatten,Activation\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.pooling import MaxPooling2D\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from datetime import timedelta\n",
        "import time\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dropout, BatchNormalization\n",
        "from keras import regularizers\n",
        "import os\n",
        "np.random.seed(666)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYxWbiieMFfG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get n randomly selected indices from the data \n",
        "def getTrainingDataIndicesOfSize(data, size):\n",
        "    chosen_indices = np.random.choice(np.arange(len(data)), size, replace=False)\n",
        "    return chosen_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt88JHm8MJHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Simple function to get a new sample from the given class\n",
        "def getNewDataOfClass(original_data, class_id):\n",
        "    chosen_index = np.random.choice(np.arange(len(original_data)), 1, replace=False)[0]\n",
        "    while(original_data[chosen_index][0] != class_id):\n",
        "        chosen_index = np.random.choice(np.arange(len(original_data)), 1, replace=False)[0]\n",
        "    return chosen_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mODpM3veMNUi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add new samples of each class until the sample-size-per-class target is reached\n",
        "def fillLowSampleCountClassesTo(original_data, indices, target_count):\n",
        "    unique, counts = np.unique(original_data[indices], return_counts=True)\n",
        "    combined_stats = np.asarray((unique, counts)).T\n",
        "    for i in range(len(combined_stats)):\n",
        "        samples_needed = target_count - combined_stats[i][1]\n",
        "        while(samples_needed > 0):\n",
        "            potential_index = getNewDataOfClass(original_data, combined_stats[i][0])\n",
        "            if not (np.any(indices[:] == potential_index)):\n",
        "                indices = np.append(indices,potential_index)\n",
        "                samples_needed -= 1\n",
        "    return indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRqoJFuOMU1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove samples of each class until the sample-size-per-class target is reached\n",
        "def deleteSamplesOverPerClassLimit(original_data, indices, target_count):\n",
        "    unique, counts = np.unique(original_data[indices], return_counts=True)\n",
        "    combined_stats = np.asarray((unique, counts)).T\n",
        "    for i in range(len(combined_stats)):\n",
        "        samples_to_delete = combined_stats[i][1] - target_count\n",
        "        while(samples_to_delete > 0):\n",
        "            potential_index = np.random.choice(np.arange(len(indices)), 1, replace=False)[0]\n",
        "            if (original_data[indices[potential_index]] == combined_stats[i][0]):\n",
        "                indices = np.delete(indices,potential_index)\n",
        "                samples_to_delete -= 1\n",
        "    return indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ar9zUFYMZc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get exactly n samples of each class randomly\n",
        "# 1. step: get n_class times sample_per_class indices (this gives approximately uniform distribution)\n",
        "# 2-3. step: correct sample-per-class differences by removing excess samples and adding to underrepresented classes\n",
        "def getRandomSamplesOfEachClassOfSize(data, n_classes, sample_per_class):\n",
        "    indices = getTrainingDataIndicesOfSize(data, n_classes*sample_per_class)\n",
        "    indices = fillLowSampleCountClassesTo(data, indices, sample_per_class)\n",
        "    indices = deleteSamplesOverPerClassLimit(data, indices, sample_per_class)\n",
        "  \n",
        "  \n",
        "    return indices\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRmpNO7tMfSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classification_report_dataframe(report, index=0):\n",
        "    report_data = []\n",
        "    lines = report.split('\\n')\n",
        "    row = {}\n",
        "\n",
        "    for line in lines[2:-5]:\n",
        "        row_data = line.split('      ')\n",
        "        class_idx = str(int(row_data[1]))\n",
        "        row[class_idx + 'precision'] = float(row_data[2])\n",
        "        row[class_idx + 'recall'] = float(row_data[3])\n",
        "        row[class_idx + 'f1_score'] = float(row_data[4])\n",
        "        #row[class_idx + 'support'] = float(row_data[5])\n",
        "    \n",
        "    dataframe = pd.DataFrame(data=row, index=[index])\n",
        "    \n",
        "    \n",
        "    #dataframe.to_csv('classification_report.csv', index = False)\n",
        "    return dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMvqdgNMM4Ir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#An example for a regularization method built in\n",
        "def getNewDropoutModel():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3,3), padding='same', input_shape=(32, 32, 3)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Conv2D(32, (3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.2))\n",
        "\n",
        "    model.add(Conv2D(64, (3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, (3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Conv2D(128, (3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Conv2D(128, (3,3), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=Adam(lr=0.0004, decay=1e-4),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIVWMD5cNEEK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run 10 iterations of the model containg dropout, save statistics\n",
        "n_classes = 10\n",
        "n_iters = 10\n",
        "\n",
        "sample_counts = [50,100,200,400,700,1000,1500,2000,3000,4000,5000]\n",
        "(X_train, Y_train), (X_test, Y_test) = cifar10.load_data()\n",
        "\n",
        "for i in range(len(sample_counts)):\n",
        "    print(\"-- Training models for sample size: \", sample_counts[i])\n",
        "    test_scores =  pd.DataFrame()\n",
        "    for iteration in range(n_iters):\n",
        "        print(\"Iteration: \", iteration)\n",
        "        sample_size = sample_counts[i]\n",
        "        model = getNewDropoutModel()\n",
        "        indices = getRandomSamplesOfEachClassOfSize(Y_train, n_classes, sample_size)\n",
        "        x_train = X_train[indices]\n",
        "        y_train = Y_train[indices]\n",
        "        x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.2, shuffle= True)\n",
        "        \n",
        "        modelpath=\"models/\" + str(sample_size) + \"/\" + str(iteration) + \"-dropout_model2-{epoch:02d}-acc-{val_acc:.4f}-loss-{val_loss:.4f}.h5\"\n",
        "        checkpoint = ModelCheckpoint(modelpath, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
        "\n",
        "        train_logpath = \"logs/\" + \"/training/\" + str(sample_size) + \"/\" + str(iteration) + \"-dropout_model.csv\"\n",
        "        train_logger = CSVLogger(train_logpath, append=False, separator=';')\n",
        "                    \n",
        "\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0,restore_best_weights=True)\n",
        "        \n",
        "        history = model.fit(x_train / 255.0, to_categorical(y_train),\n",
        "                  batch_size=128,\n",
        "                  shuffle=True,\n",
        "                  epochs=200,\n",
        "                  validation_data=(x_valid / 255.0, to_categorical(y_valid)),\n",
        "                  callbacks=[keras.callbacks.History(), checkpoint, train_logger, early_stopping],\n",
        "                           verbose = 0)\n",
        "        \n",
        "        score = model.evaluate(X_test / 255.0, to_categorical(Y_test), batch_size=128, verbose=0)\n",
        "        score = dict(zip(model.metrics_names, score))\n",
        "        score_df = pd.DataFrame(data=score, index=[0])\n",
        "        y_pred = model.predict_classes(X_test)\n",
        "        report = classification_report(Y_test, y_pred)\n",
        "        report_df = classification_report_dataframe(report)\n",
        "        test_scores = test_scores.append(pd.concat([score_df, report_df], axis=1))\n",
        "        \n",
        "    test_logpath = \"logs/\" + \"/test/\" + str(sample_size) + \"/\"+ \"dropout_model.csv\"\n",
        "    test_scores.to_csv(test_logpath)\n",
        "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(test_scores['acc']), np.std(test_scores['acc'])))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}